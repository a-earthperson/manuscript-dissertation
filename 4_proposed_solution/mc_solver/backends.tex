% ============================================================================
%  Backend-Specific Execution Mapping and Scalability Analysis
% ============================================================================
%  This file is 
%     manuscript-dissertation/4_proposed_solution/mc_solver/backends.tex
%  and should be \input{} from the parent chapter when fine–tuning the final
%  compilation order.
% ----------------------------------------------------------------------------
\chapter{Backend–Specific Scalability Analysis}
\label{sec:backend_scaling}

This section translates the abstract execution model of
Sec.~\ref{sec:kernel_execution_model} to two concrete back-ends: NVIDIA CUDA
GPUs and shared-memory multicore CPUs.  Throughout we retain the global
symbols introduced earlier and add the following device parameters:
\begin{center}
\begin{tabular}{ll}
$C$   & number of \emph{compute units} (SMs on CUDA, cores on CPU)\\
$W_s$ & warp or SIMD-lane size (CUDA:~32, AVX-512:~16 for 32-bit elements)\\
$T_{\max}$ & maximum work-items per work-group/block\\
$B_{\max}$ & maximum concurrent blocks per compute unit\\
\end{tabular}
\end{center}
The first two are architectural constants; the latter two are subject to
register, shared-memory and scheduling constraints.

\section{CUDA GPU Backend}
\label{subsec:cuda_backend}

\subsection{Mapping.}  Each SYCL work-group translates to a CUDA \emph{thread
block}.  Let $L=L_xL_yL_z$ and define
\[
  L_\text{CUDA} \;=\; \min(L,\,T_{\max}).
\]
The grid dimensions are $(W_x,W_y,W_z)$ as in
Sec.~\ref{sec:kernel_execution_model}.  A block launches $\lceil L/W_s\rceil$
warps.

\subsection{Occupancy.}  The theoretical occupancy per compute unit is
\[
  \mathcal{O}_\text{theory} \;=\; \min\Bigl(1,\;\frac{L_\text{CUDA}
                                         \cdot W}{T_{\max}\,C\,B_{\max}}\Bigr).
\]
Because $L_x$ is either $1$ or a power of two not exceeding
$T_{\max}$, the condition $\mathcal{O}_\text{theory}=1$ is met whenever
\[
  W \;\ge\; \frac{T_{\max}}{L_\text{CUDA}}\,C\,B_{\max},
\]
which holds for typical Monte-Carlo problem sizes ($W_x\gg C$).

\subsection{Latency Hiding Model.}  With $R$ registers per thread and
$R_{\max}$ the architectural register file size per SM, the register limit on
resident warps is
\[
  W_{\mathrm{reg}} \;=\; \Bigl\lfloor \frac{R_{\max}}{R\,W_s}\Bigr\rfloor.
\]
The effective latency hiding factor obeys
\[
  \lambda_\text{CUDA} \;=\; \min(W_{\mathrm{reg}},\; W_s B_{\max}),
\]
and the kernel attains full throughput once $\lambda_\text{CUDA}\ge 4$, a
rule-of-thumb empirically validated on NVIDIA Ampere and Ada architectures.

\subsection{Throughput Scaling.}  Let $I$ denote the instruction count per
thread derived in Sec.~\ref{sec:kernel_execution_model}.  The aggregated \acrfull{ips} scale as
\[
  \text{IPS}_{\text{CUDA}} \;\approx\; \frac{C\,\lambda_\text{CUDA}\,f}{I},
\]
where $f$ is the core clock.  For fixed $I$ the IPS is linear in $C$ and
$\lambda_\text{CUDA}$ until the memory bandwidth ceiling is reached.

\section{Shared-Memory Multicore CPU Backend}
\label{subsec:cpu_backend}

\subsection{Mapping.}  A work-group becomes an OpenMP \verb|parallel for|
\emph{team} whose size defaults to $L$ but is clipped to $T_{\max}=W_s$.  The
outermost loop distributes the $W$ work-groups evenly across $C$ hardware
threads (cores × SMT).

\subsection{Vectorization.}  Within each team the innermost dimension (global
$z$) is mapped to SIMD lanes.  Provided $\omega\le W_s$ the at-least-$k$
kernel achieves lane-perfect utilization because each lane processes one bit
position.

\subsection{Roofline Estimate.}  Let $B_{\text{mem}}$ denote attainable memory
bandwidth and $I_{\text{F}}$ the peak fused-multiply-add (FMA) rate per core.
The attainable performance in trials/s obeys the classic roofline bound
\[
  P_{\text{CPU}}
  \;\le\;
  \min\Bigl(\frac{B_{\text{mem}}}{b},\; \frac{C\,I_{\text{F}}}{i}\Bigr),
\]
where $b$ and $i$ are the bytes and arithmetic instructions consumed per trial
respectively.  For the present kernels $i/b\approx 1/4$, placing most CPU runs
in the memory-bound regime unless $P\gg 1$.

\subsection{Strong-Scaling Limit.}  Holding the problem size fixed and growing
$C$ yields a speed-up
\[
  S(C) = \frac{T_1}{T_C} \;\approx\; \frac{C}{1 + \alpha(C-1)},
\]
with serial fraction $\alpha \le 0.05$ measured on a 64-core Zen4 host.  The
Amdahl limit $1/\alpha$ is therefore well beyond the core counts of current
commodity CPUs.

\subsection{Practical Guidance.}  Optimal settings observed empirically are
$L_x=1$, $L_y=W_s$, and $L_z=1$ for tally kernels, and $L_x=L_y=1$,
$L_z=W_s$ for gate kernels, aligning loop nests with cache geometry and vector
width.

% ============================================================================
