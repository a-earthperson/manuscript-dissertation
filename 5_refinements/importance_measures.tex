%---------------------------------------------------------------------------
%  Monte-Carlo Importance Measures
%---------------------------------------------------------------------------
\chapter{Monte-Carlo Evaluation of Importance Measures}
\label{sec:mc_importance_measures}

Reliability practitioners rarely stop at a mere point estimate of the top‐event
probability.  Once a probabilistic directed acyclic graph (\acrshort{pdag}) has
been quantified, the next natural question is 
\emph{``which basic events matter the most?''}.  Importance measures translate
raw probabilities into actionable rankings that drive maintenance decisions,
design improvements, and risk communication.  Classical definitions dating back
to Birnbaum, Fussell--Vesely and Vesely are revisited in
\S\ref{sec:foundations_importance} of this dissertation.  The present section
extends those definitions to the Monte-Carlo solver introduced in
Chapter~\ref{chap:mc_solver} and details how the required statistics are
gathered, reduced, and reported \emph{without} incurring additional sampling
runs.

\section*{Notation.}  Let $Z\in\{0,1\}$ be the indicator of system failure
(i.e.~the value of the \emph{root} gate) and let $X_i\in\{0,1\}$ denote the
state of basic event~$i$.
A single Monte–Carlo iteration produces a batch of $N$ Bernoulli trials
(\S\ref{subsec:be_kernel}); repeating the experiment for $T$ iterations yields
$T N$ 
independent samples $\bigl\{\,(Z^{(t,j)},\,X_i^{(t,j)})\bigr\}$ with indices
$t\!=\!1\dots T$ (iteration) and $j\!=\!1\dots N$ (trial within an
iteration).

%--------------------------------------------------------------------------
\section{Minimal Sufficient Statistics}
%--------------------------------------------------------------------------
For \emph{each} basic event the Monte-Carlo engine maintains exactly three
counters:
\begin{equation}
    \label{eq:basic_counters}
    \begin{aligned}
        s_i      &\;= \sum_{t,j}         X_i^{(t,j)}                                  & (\text{one--bits of }X_i)\\
        s_0      &\;= \sum_{t,j}         Z^{(t,j)}                                   & (\text{one--bits of }Z)\\
        s_{0,i}  &\;= \sum_{t,j} Z^{(t,j)} X_i^{(t,j)}                               & (\text{joint one--bits})
    \end{aligned}
\end{equation}
plus the common sample size $n\!=\!T N$.  Eq.~\eqref{eq:basic_counters}
constitutes a \emph{minimal sufficient} set for all first--order importance
measures considered herein: every statistic can be expressed as a function of
$(s_0,\,s_i,\,s_{0,i},\,n)$.

The counters are accumulated on–device during the tally stage
(\S\ref{sec:tally_kernel}).  Line~\texttt{popcount(root \&\& event)} adds a
single `\texttt{\&}` and `\texttt{popcount}` instruction per sample word, yet
obviates the need for any post–simulation reprocessing.

%--------------------------------------------------------------------------
\section{Estimators for Classical Measures}
%--------------------------------------------------------------------------
Define the unbiased estimators
\[\widehat{p}_0 = \frac{s_0}{n}, \quad \widehat{p}_i = \frac{s_i}{n},
   \quad \widehat{p}_{0,i} = \frac{s_{0,i}}{n}.\]

\subsection{Birnbaum marginal importance (MIF).}
For coherent systems the Birnbaum index equals the covariance scaled by the
component variance:
\begin{equation}
   \operatorname{MIF}_i
   \;=\; \frac{\operatorname{Cov}(Z, X_i)}{\operatorname{Var}(X_i)}
   \;=\; \frac{\widehat{p}_{0,i} - \widehat{p}_0\widehat{p}_i}
                {\,\widehat{p}_i\bigl(1-\widehat{p}_i\bigr)}.
   \label{eq:mif_estimator}
\end{equation}
If $\widehat{p}_i$ is close to $0$ or $1$ a small pseudo–count $\epsilon$ is
added to avoid numerical blow‐up.

\subsection{Critical importance (CIF).}
CIF normalizes MIF by the top–event probability
\[ \operatorname{CIF}_i \;=\; \frac{\operatorname{MIF}_i\,\widehat{p}_i}
                                     {\widehat{p}_0}.\]
\subsection{Diagnostic importance (DIF).}
\[ \operatorname{DIF}_i \;=\;
   \frac{\widehat{p}_{0,i}}{\widehat{p}_0\,\widehat{p}_i}.\]
\subsection{Risk achievement (RAW) \&\ reduction worth (RRW).}
Set $p_0^{(i=1)} = \widehat{p}_{0,i}/\widehat{p}_i$ and
$p_0^{(i=0)} = \bigl(\widehat{p}_0 - \widehat{p}_{0,i}\bigr)/(1-\widehat{p}_i)$.
Then
\begin{align*}
  \operatorname{RAW}_i &= \frac{p_0^{(i=1)}}{\widehat{p}_0}, &
  \operatorname{RRW}_i &= \frac{\widehat{p}_0}{p_0^{(i=0)}}.
\end{align*}
All numerators and denominators derive directly from the counters in
Eq.~\eqref{eq:basic_counters}.

%--------------------------------------------------------------------------
\section{Confidence Intervals}
%--------------------------------------------------------------------------
Because the estimators are smooth functions of sample proportions, the \acrshort{clt}
allows a delta–method approximation.  Denote by $\boldsymbol{s}=(s_0,s_i,s_{0,i})^T$ and
let $g(\boldsymbol{s})$ be one of the above measures.
The first‐order Taylor expansion around the expectation yields
\[
  \operatorname{Var}\bigl[g(\boldsymbol{s})\bigr]
  \;\approx\;
  \nabla g^T \!\,\Sigma\, \nabla g,
\]
where $\Sigma$ is the $3\!\times\!3$ covariance matrix of $(s_0,s_i,s_{0,i})$.
Closed‐form expressions are lengthy but straightforward; Appendix
\ref{app:importance_ci} lists them and the Monte‐Carlo implementation evaluates
those formulas on the host side whenever intermediate confidence bands are
requested.

%--------------------------------------------------------------------------
\section{Layered Evaluation Algorithm}
%--------------------------------------------------------------------------
\begin{itemize}
  \item \textbf{Sampling} – basic‐event kernel generates bit–packed outcomes
    $X_i^{(t,\cdot)}$ and stores them in device buffers.
  \item \textbf{Gate evaluation} – logic kernels propagate the state upward and
    write the root buffer $Z^{(t,\cdot)}$.
  \item \textbf{Tally popcount} – the tally kernel counts $s_i$ and $s_0$ via
    `\texttt{popcount}` operations.
  \item \textbf{Joint accumulation} – in the \emph{same work‐item} the bitwise
    \textsc{and} $Z \& X_i$ is popped‐counted to obtain $s_{0,i}$.
  \item \textbf{Host reduction} – after iteration~$t$ the per–group partials
    are atomically accumulated; the host holds only updated scalars.
  \item \textbf{Importance update} – when invoked, the host computes
    Eqs.~\eqref{eq:mif_estimator}–\eqref{eq:diagnostic} and their confidence
    intervals without any device traffic.
\end{itemize}

%--------------------------------------------------------------------------
\section{Alternate Evaluation Strategies}
%--------------------------------------------------------------------------
\subsection{Finite–difference XOR.}  A popular textbook derivation expresses the
Birnbaum index as the exclusive‐or of two counterfactual evaluations
$F\bigl(X_i\!=\!1\bigr)$ and $F\bigl(X_i\!=\!0\bigr)$.  Implementing this idea
literally requires \emph{two} additional gate passes \emph{per variable}.  For
large fault trees the quadratic cost outweighs any conceptual elegance.

\subsection{Post‐hoc analysis kernels.}  One may postpone the computation of
$s_{0,i}$ until the user demands an importance report.  A dedicated kernel then
reads the stored buffers, performs the \textsc{and}+\textsc{popcount}, and
returns the joint counts.  This variant doubles memory traffic but leaves the
critical sampling path untouched and allows on‐demand higher–order statistics
(e.g.~covariance matrices).  In the solver implementation the feature is hidden
behind the command‐line flag \verb|--mc-extra-stats|.

\subsection{Default design choice.}  The in–tally covariance accumulation incurs
\emph{one} extra integer instruction per 64 Bernoulli trials and scales to
thousands of variables with negligible overhead.  It therefore remains the
solver’s default, while post–hoc kernels serve as an opt-in diagnostic tool.

%--------------------------------------------------------------------------
\section{Illustrative Example}
%--------------------------------------------------------------------------
Consider the sample \acrshort{pdag} in Fig.~\ref{fig:demo_pdag} with four basic
events.  A Monte–Carlo campaign of $T=10^4$ iterations, $B=64$ batches and
$P=16$ bit–packs ($n=6.55\times10^6$ trials) produces the counters in
Table~\ref{tab:demo_counters}.  Plugging those values into the formulas above
yields the importance ranking shown in
Table~\ref{tab:demo_importance}.  Event~$e_3$ dominates every metric, which
confirms the analytical intuition that it is the unique single‐point failure of
the redundant subsystem.

Through a single, inexpensive extension of the tally kernel the Monte–Carlo
solver gathers all statistics needed for first–order importance measures.
Derived quantities and their confidence intervals are evaluated on the host
with $\mathcal{O}(\lvert\text{events}\rvert)$ arithmetic and \emph{no} extra
device passes.  The approach reconciles statistical rigor with GPU efficiency
and serves as the backbone for the risk‐ranking features demonstrated in
Chapter~\ref{chap:case_studies}.
