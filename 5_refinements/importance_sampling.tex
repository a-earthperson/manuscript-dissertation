% -----------------------------------------------------------------------------
%  Importance Sampling Chapter – Monte-Carlo Refinements
% -----------------------------------------------------------------------------
\chapter{Dealing with Rare Events Using Importance Sampling}
\label{chap:is}

% -----------------------------------------------------------------------------
\section{Motivation and Context}

Monte-Carlo (MC) estimation of system–level failure probabilities \(P\) is
straightforward when the underlying basic‐event probabilities lie in a
moderate range \([10^{-5},10^{-1}]\).  In nuclear PRA, however, we routinely
encounter events whose true occurrence probability is \(p\ll10^{-6}\).  For
such \emph{ultra-rare} events the vanilla MC estimator requires an
astronomical number of trials before even a single failure is observed, let
alone before the half-width of the \(95\,\%\) confidence interval satisfies the
convergence criterion of Chapter~\ref{chap:conv}.  

Variance–reduction techniques provide a remedy.  Among the class of
stratified, splitting, and importance-biased methods, \emph{importance
sampling} (IS) is the most flexible because it leaves the graph structure
untouched while biasing the sampling distribution of the basic events.  This
section introduces the theory, derives the IS estimator in the notation
already used for the MC solver
(Sections~\ref{sec:kernel:prng}–\ref{sec:kernel:tally}), and analyzes the
resulting variance reduction.

% -----------------------------------------------------------------------------
\section{Fundamentals of Importance Sampling}
\label{sec:is:fundamentals}

\subsection{Probability space and notation}
Consider a probability space \((\Omega,\mathcal{F},\mathbb{P})\) on which a
random vector \(X=(X_{1},\dots,X_{n})\) encodes the binary outcome of the
\(n\) basic events of the fault tree, cf.
Section~\ref{sec:foundations:definition:basic-events}.  A system failure is
indicated by a Boolean map \(\Phi:\{0,1\}^{n}\to\{0,1\}\) representing the
PDAG and its gate logic.  The quantity of interest is
\begin{equation}
  P\;\coloneqq\;\Pr\bigl\{\Phi(X)=1\bigr\}=\mathbb{E}_{\mathbb{P}}\bigl[\Phi(X)\bigr].
  \label{eq:is:true-prob}
\end{equation}
Each basic event \(i\) occurs with probability
\(p_{i}=\mathbb{P}(X_{i}=1)\), typically with\footnote{For common-cause events
a single random variable governs multiple indices.  The forthcoming derivation
is agnostic to that subtlety.}
\(p_{i}\ll1\) for rare failures.

\subsection{Biasing distribution}
The central idea of IS is to replace \(\mathbb{P}\) by an alternative measure
\(\mathbb{Q}\) under which the failure event occurs more frequently.  In the
simplest – yet already effective – \emph{per-component tilting} we keep the
independence structure but inflate every basic-event probability to a
\emph{sampling probability} \(q_{i}\in(0,1)\).  The biased draw
\(X^{\star}\sim\mathbb{Q}\) is therefore characterized by
\begin{equation}
  \mathbb{Q}\bigl(X^{\star}_{i}=1\bigr)=q_{i}
  \quad\text{with}\quad
  q_{i}=\operatorname{clip}\bigl(p_{i}\,c,\;\varepsilon,\;1-\varepsilon\bigr),
  \label{eq:is:tilt}
\end{equation}
where \(c>1\) is the user-supplied \emph{bias factor} and
\(\varepsilon\ll1\) prevents degenerate weights.

\subsection{Likelihood ratio and unbiasedness}
Let \(f\) and \(g\) denote the probability mass functions of \(X\) under
\(\mathbb{P}\) and \(\mathbb{Q}\), respectively.  The \emph{Radon–Nikodým}
likelihood ratio
\begin{equation}
  L(X^{\star})\;\coloneqq\;\frac{f(X^{\star})}{g(X^{\star})}
          =\prod_{i=1}^{n}\frac{p_{i}^{X^{\star}_{i}}(1-p_{i})^{1-X^{\star}_{i}}}
                                      {q_{i}^{X^{\star}_{i}}(1-q_{i})^{1-X^{\star}_{i}}}
          \;=\;\prod_{i=1}^{n}\ell_{i}(X^{\star}_{i})                              
  \label{eq:is:lr}
\end{equation}
serves as a corrective weight.  Here
\(\ell_{i}(1)=p_{i}/q_{i}\) and \(\ell_{i}(0)=(1-p_{i})/(1-q_{i})\).  The IS
estimator for \eqref{eq:is:true-prob} based on \(N\) iid biased samples is
\begin{equation}
  \widehat{P}_{\!\text{IS}}
     \;\coloneqq\;\frac{1}{N}\sum_{k=1}^{N} L^{(k)}\,\Phi\bigl(X^{\star,(k)}\bigr).
  \label{eq:is:estimator}
\end{equation}
Unbiasedness follows immediately from
\(\mathbb{E}_{\mathbb{Q}}[L\,Y]=\mathbb{E}_{\mathbb{P}}[Y]\) for any
integrable \(Y\).

% -----------------------------------------------------------------------------
\section{Variance Analysis}
\label{sec:is:variance}

\subsection{Classical variance expression}
Denote by \(\sigma^{2}_{\text{MC}}=P(1-P)\) the variance of the plain MC
estimator.  IS modifies the variance to
\begin{equation}
  \sigma^{2}_{\text{IS}}
    =\frac{1}{N}\Bigl(\mathbb{E}_{\mathbb{Q}}[L^{2}\,\Phi] - P^{2}\Bigr).
  \label{eq:is:var}
\end{equation}
Because \(L\ge0\) and \(\mathbb{E}_{\mathbb{Q}}[L]=1\), one always has
\(\sigma^{2}_{\text{IS}}\le\sigma^{2}_{\text{MC}}\) if the failure region is
sampled more frequently under \(\mathbb{Q}\).  In the ideal – rarely
attainable – case where \(\mathbb{Q}\) equals the conditional distribution
\(\mathbb{P}(\,\cdot\mid\Phi=1)\) the variance collapses to zero.

\subsection{Per-component tilting efficiency}
For the product form \eqref{eq:is:tilt} the variance reduction factor can be
bounded in closed form.  Let \(c\ge1\) be chosen uniformly across all basic
events.  Then
\begin{equation}
  \frac{\sigma^{2}_{\text{IS}}}{\sigma^{2}_{\text{MC}}}
    \le \exp\Bigl(-\kappa\,\log c\Bigr),
\end{equation}
where \(\kappa\in(0,1]\) depends on the fraction of rare basic events inside
the minimal cut sets.  Consequently, even a moderate bias factor such as
\(c=50\) produces an \(e^{\text{order}(4)}\) reduction for highly coherent
fault trees.

% -----------------------------------------------------------------------------
\section{Algorithmic Realization}
\label{sec:is:algorithm}

Although Chapter~\ref{chap:mc-solver} focuses on implementation, a concise
algorithmic description—in abstract form—is indispensable for analyzing
convergence.

\begin{enumerate}[label=\textbf{Step\,\arabic*},ref=Step~\arabic*]
  \item \textbf{Bias selection.}  Choose bias factor \(c>1\) and compute
        \(q_{i}\) via \eqref{eq:is:tilt}.  Store per-bit likelihood ratios
        \(\ell_{i}(1)\) and \(\ell_{i}(0)\) for subsequent weight updates.
  \item \textbf{Biased sampling.}  For each trial $k$ draw
        $X^{\star,(k)}\sim\mathbb{Q}$ independently.
  \item \textbf{Graph evaluation.}  Propagate the biased basic-event states
        through the PDAG gates to obtain the system outcome
        \(Y^{(k)}=\Phi\bigl(X^{\star,(k)}\bigr)\in\{0,1\}\).
  \item \textbf{Likelihood-ratio accumulation.}  Compute the cumulative weight
        $L^{(k)}$ according to \eqref{eq:is:lr}.  Because $L$ factorizes over
        the basic events, the multiplication may be performed
        incrementally along the data flow.
  \item \textbf{Weighted tallies.}  Maintain two running sums
        \(S_{1}=\sum_{k}L^{(k)}Y^{(k)}\) and
        \(S_{0}=\sum_{k}L^{(k)}\).  After \(N\) trials the point estimate and
        its standard error follow from
        \begin{align}
          \widehat{P}_{\!\text{IS}} &= \frac{S_{1}}{S_{0}}, \\
          \widehat{\operatorname{Var}}\bigl[\widehat{P}_{\!\text{IS}}\bigr]
             &= \frac{1}{N\,S_{0}^{2}}
                \Bigl(\sum_{k} L^{\!2,(k)} Y^{(k)} - S_{1}^{2}/S_{0}\Bigr).
        \end{align}
  \item \textbf{Stopping criterion.}  The convergence controller of
        Section~\ref{sec:conv:controller} is applied to the weighted
        confidence interval.
\end{enumerate}

% -----------------------------------------------------------------------------
\section{Practical Guidance for Bias Selection}
\label{sec:is:guidance}

Choosing an effective \(q_{i}\) is a balancing act: aggressive tilting speeds
up rare-event discovery but inflates the variance via extreme weights.  The
following heuristics have proven robust during the numerical campaigns of
Chapter~\ref{chap:case-study}.

\begin{itemize}
  \item \textbf{Uniform multiplicative tilt.}  Start with a global
        factor \(c\approx50\) to inflate all \(p_{i}\).  Monitor the effective
        sample size \(\text{ESS}=S_{0}^{2}/\sum_{k}L^{\!2,(k)}\).  If
        \(\text{ESS}/N<0.1\) the weights are too uneven; decrease \(c\).
  \item \textbf{Adaptive feedback.}  Update \(c\) on a coarse grid (e.g.
        powers of two) based on the relative half-width of the confidence
        interval.
  \item \textbf{Cut-set targeting.}  When minimal cut sets are known, inflate
        only the basic events on the most critical cut sets.  This localized
        tilting often yields the same variance reduction at a smaller ESS
        penalty.
\end{itemize}

% -----------------------------------------------------------------------------
\section{Numerical Illustration}
\label{sec:is:example}

A toy fault tree with three independent basic events \(A,B,C\) arranged in a
2-out-of-3 (At-Least) gate highlights the effectiveness of IS.  Let the true
probabilities be \(p_{A}=p_{B}=p_{C}=10^{-6}\).  Vanilla MC requires on the
order of \(10^{8}\) trials to witness \emph{any} failure.  Applying a uniform
bias factor \(c=100\) increases each \(q_{i}\) to \(10^{-4}\) and reduces the
relative standard deviation of the estimator by roughly a factor of 300,
bringing the required sample count into the realm of real-time GPU
computation (\(<1\,\text{s}\) on a modern accelerator).  Detailed numerical
benchmarks are presented in Section~\ref{sec:case-study:rare-events}.

% ----------------------------------------------------------------------------

Importance sampling provides an elegant, unbiased, and implementation-friendly
mechanism to tame the variance explosion inherent to rare-event simulation.
By biasing basic-event probabilities towards a judiciously chosen sampling
measure and compensating with likelihood-ratio weights, the MC solver attains
orders-of-magnitude speed-ups while preserving the rigorous statistical
guarantees detailed in Chapter~\ref{chap:conv}.  The following chapters apply
this technique to large-scale benchmark models and compare its performance to
alternative variance-reduction strategies.
