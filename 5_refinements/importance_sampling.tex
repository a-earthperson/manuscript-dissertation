% -----------------------------------------------------------------------------
%  Importance Sampling Chapter – Monte-Carlo Refinements
% -----------------------------------------------------------------------------
\chapter{Dealing with Rare Events Using Importance Sampling}
\label{chap:is}

% -----------------------------------------------------------------------------
\section{Motivation and Context}

Monte-Carlo (MC) estimation of system–level failure probabilities \(P\) is
straightforward when the underlying basic‐event probabilities lie in a
moderate range \([10^{-5},10^{-1}]\).  In nuclear PRA, however, we routinely
encounter events whose true occurrence probability is \(p\ll10^{-6}\).  For
such \emph{ultra-rare} events the vanilla MC estimator requires an
astronomical number of trials before even a single failure is observed, let
alone before the half-width of the \(95\,\%\) confidence interval satisfies the
convergence criterion of Chapter~\ref{sec:convergence_criterion}.  

Variance–reduction techniques provide a remedy.  Among the class of
stratified, splitting, and importance-biased methods, \emph{importance
sampling} (IS) is the most flexible because it leaves the graph structure
untouched while biasing the sampling distribution of the basic events.  This
section introduces the theory, derives the IS estimator in the notation
already used for the MC solver
(Section~\ref{sec:bitpack-prob-sampling}, Chapter \ref{sec:tally_kernel}), and analyzes the
resulting variance reduction.

% -----------------------------------------------------------------------------
\section{Fundamentals of Importance Sampling}
\label{sec:is:fundamentals}

\subsection{Probability Space Notation}
Consider a probability space \((\Omega,\mathcal{F},\mathbb{P})\) on which a
random vector \(X=(X_{1},\dots,X_{n})\) encodes the binary outcome of the
\(n\) basic events of the fault tree, cf.
Section~\ref{sec:fault_tree_definition}.  A system failure is
indicated by a Boolean map \(\Phi:\{0,1\}^{n}\to\{0,1\}\) representing the
PDAG and its gate logic.  The quantity of interest is
\begin{equation}
  P\;\coloneqq\;\Pr\bigl\{\Phi(X)=1\bigr\}=\mathbb{E}_{\mathbb{P}}\bigl[\Phi(X)\bigr].
  \label{eq:is:true-prob}
\end{equation}
Each basic event \(i\) occurs with probability
\(p_{i}=\mathbb{P}(X_{i}=1)\), typically with\footnote{For common-cause events
a single random variable governs multiple indices.  The forthcoming derivation
is agnostic to that subtlety.}
\(p_{i}\ll1\) for rare failures.

\subsection{Biasing Distribution}
The central idea of IS is to replace \(\mathbb{P}\) by an alternative measure
\(\mathbb{Q}\) under which the failure event occurs more frequently.  In the
simplest – yet already effective – \emph{per-component tilting} we keep the
independence structure but inflate every basic-event probability to a
\emph{sampling probability} \(q_{i}\in(0,1)\).  The biased draw
\(X^{\star}\sim\mathbb{Q}\) is therefore characterized by
\begin{equation}
  \mathbb{Q}\bigl(X^{\star}_{i}=1\bigr)=q_{i}
  \quad\text{with}\quad
  q_{i}=\operatorname{clip}\bigl(p_{i}\,c,\;\varepsilon,\;1-\varepsilon\bigr),
  \label{eq:is:tilt}
\end{equation}
where \(c>1\) is the user-supplied \emph{bias factor} and
\(\varepsilon\ll1\) prevents degenerate weights.

\subsection{Likelihood ratio and unbiasedness}
Let \(f\) and \(g\) denote the probability mass functions of \(X\) under
\(\mathbb{P}\) and \(\mathbb{Q}\), respectively.  The \emph{Radon–Nikodým}
likelihood ratio
\begin{equation}
  L(X^{\star})\;\coloneqq\;\frac{f(X^{\star})}{g(X^{\star})}
          =\prod_{i=1}^{n}\frac{p_{i}^{X^{\star}_{i}}(1-p_{i})^{1-X^{\star}_{i}}}
                                      {q_{i}^{X^{\star}_{i}}(1-q_{i})^{1-X^{\star}_{i}}}
          \;=\;\prod_{i=1}^{n}\ell_{i}(X^{\star}_{i})                              
  \label{eq:is:lr}
\end{equation}
serves as a corrective weight.  Here
\(\ell_{i}(1)=p_{i}/q_{i}\) and \(\ell_{i}(0)=(1-p_{i})/(1-q_{i})\).  The IS
estimator for \eqref{eq:is:true-prob} based on \(N\) iid biased samples is
\begin{equation}
  \widehat{P}_{\!\text{IS}}
     \;\coloneqq\;\frac{1}{N}\sum_{k=1}^{N} L^{(k)}\,\Phi\bigl(X^{\star,(k)}\bigr).
  \label{eq:is:estimator}
\end{equation}
Unbiasedness follows immediately from
\(\mathbb{E}_{\mathbb{Q}}[L\,Y]=\mathbb{E}_{\mathbb{P}}[Y]\) for any
integrable \(Y\).

% -----------------------------------------------------------------------------
\section{Variance Analysis}
\label{sec:is:variance}

\subsection{Classical variance expression}
Denote by \(\sigma^{2}_{\text{MC}}=P(1-P)\) the variance of the plain MC
estimator.  IS modifies the variance to
\begin{equation}
  \sigma^{2}_{\text{IS}}
    =\frac{1}{N}\Bigl(\mathbb{E}_{\mathbb{Q}}[L^{2}\,\Phi] - P^{2}\Bigr).
  \label{eq:is:var}
\end{equation}
Because \(L\ge0\) and \(\mathbb{E}_{\mathbb{Q}}[L]=1\), one always has
\(\sigma^{2}_{\text{IS}}\le\sigma^{2}_{\text{MC}}\) if the failure region is
sampled more frequently under \(\mathbb{Q}\).  In the ideal – rarely
attainable – case where \(\mathbb{Q}\) equals the conditional distribution
\(\mathbb{P}(\,\cdot\mid\Phi=1)\) the variance collapses to zero.

\subsection{Per-component tilting efficiency}
For the product form \eqref{eq:is:tilt} the variance reduction factor can be
bounded in closed form.  Let \(c\ge1\) be chosen uniformly across all basic
events.  Then
\begin{equation}
  \frac{\sigma^{2}_{\text{IS}}}{\sigma^{2}_{\text{MC}}}
    \le \exp\Bigl(-\kappa\,\log c\Bigr),
\end{equation}
where \(\kappa\in(0,1]\) depends on the fraction of rare basic events.

% -----------------------------------------------------------------------------
\section{Algorithmic Realization}
\label{sec:is:algorithm}

Although Chapter~\ref{chap:mc_solver} focuses on implementation, a concise
algorithmic description is indispensable for analyzing
convergence.

\begin{enumerate}[label=\textbf{Step\,\arabic*},ref=Step~\arabic*]
  \item \textbf{Bias selection.}  Choose bias factor \(c>1\) and compute
        \(q_{i}\) via \eqref{eq:is:tilt}.  Store per-bit likelihood ratios
        \(\ell_{i}(1)\) and \(\ell_{i}(0)\) for subsequent weight updates.
  \item \textbf{Biased sampling.}  For each trial $k$ draw
        $X^{\star,(k)}\sim\mathbb{Q}$ independently.
  \item \textbf{Graph evaluation.}  Propagate the biased basic-event states
        through the PDAG gates to obtain the system outcome
        \(Y^{(k)}=\Phi\bigl(X^{\star,(k)}\bigr)\in\{0,1\}\).
  \item \textbf{Likelihood-ratio accumulation.}  Compute the cumulative weight
        $L^{(k)}$ according to \eqref{eq:is:lr}.  Because $L$ factorizes over
        the basic events, the multiplication may be performed
        incrementally along the data flow.
  \item \textbf{Weighted tallies.}  Maintain two running sums
        \(S_{1}=\sum_{k}L^{(k)}Y^{(k)}\) and
        \(S_{0}=\sum_{k}L^{(k)}\).  After \(N\) trials the point estimate and
        its standard error follow from
        \begin{align}
          \widehat{P}_{\!\text{IS}} &= \frac{S_{1}}{S_{0}}, \\
          \widehat{\operatorname{Var}}\bigl[\widehat{P}_{\!\text{IS}}\bigr]
             &= \frac{1}{N\,S_{0}^{2}}
                \Bigl(\sum_{k} L^{\!2,(k)} Y^{(k)} - S_{1}^{2}/S_{0}\Bigr).
        \end{align}
  \item \textbf{Stopping criterion.}  The convergence controller of
        Section~\ref{sec:convergence_criterion} is applied to the weighted
        confidence interval.
\end{enumerate}
