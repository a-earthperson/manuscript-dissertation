
\section*{An Informal Overview}

\paragraph*{Why another PRA solver?}
Probabilistic risk assessment for large nuclear systems asks a simple
question: \emph{how likely is an accident, given thousands of component
failures, recoveries, and human actions?} The textbook solution (build a decision diagram, or enumerate every failure combination) scales exponentially and quickly becomes impractical. Over the years analysts have tamed that
combinatorial surge with gate restrictions, bounding tricks, and
rare-event approximations; the price has been either coarse error
margins or hours‐long runtimes.


\paragraph*{A different stance: sample first, ask questions later.}
Instead of traversing every Boolean state, we draw random global
scenarios and evaluate the entire logic graph in one pass.
The key enabler is hardware parallelism: by packing \(64\)
Bernoulli trials into a 64-bit word we turn Boolean gates into
bitwise instructions and let modern GPUs churn through billions of
scenarios per second. The solver, \textsc{mcScram}, views the complete
PRA model (nested event trees, fault trees, and their cross-links) as
one unified probabilistic directed acyclic graph (PDAG) that is sampled
and tallied en bloc.

\paragraph*{What falls out of that design?}
\begin{itemize}
  \item \textbf{Flexibility.}  Any gate expressible in Boolean logic
        (NOT, \(k\)-out-of-\(n\) voting, XOR, Cardinality, \dots) is handled
        without extra coding.
  \item \textbf{Speed.}  On a mid-range laptop GPU the method converges
        to sub-percent error for graphs with a few thousand events in
        \(\lesssim 5\) seconds.
  \item \textbf{Extensibility.}  Because sampling is the only primitive,
        common-cause failures, importance measures, and importance
        sampling plug in naturally.
\end{itemize}

\paragraph*{What it is \emph{not}.}
The approach is still Monte-Carlo: for events rarer than
\(10^{-8}\) additional variance-reduction techniques are required, and
maximum throughput depends on access to a GPU or other SIMD hardware.


\paragraph*{Road map.}
\begin{itemize}
  \item \textbf{Chapter 4 (Part I).} Formalizes PRA models as probabilistic DAGs. 
  \item \textbf{Chapters 7–11 (Part III).} Present the data-parallel Monte-Carlo engine—sampling, gate kernels, tallying, and backend scalability. 
  \item \textbf{Chapters 13–18 (Part IV).} Detail refinements: randomness guarantees, composite convergence policy, common-cause failure handling, importance measures, importance sampling, and hardware-native voting gates. 
  \item \textbf{Chapters 12 and 19.} Benchmark accuracy, runtime, and structural compression on the Aralia suite. 
  \item \textbf{Part V.} Outlines future “inverse-problem’’ directions such as parameter fitting.
\end{itemize}

\clearpage

% \vspace{-4mm}
% \subsection*{Core Rationale - Building a Probability Estimator}
% We propose a data-parallel Monte Carlo (MC) scheme that samples directly from input component-reliability distributions. Rather than generating every possible failure combination, we focus on random draws of the system’s global state, then tally how often particular sequences lead to an end state of interest. This sampling-based approach trades the intrinsic explosion of inclusion-exclusion terms with an exponential number of sample draws. This approach can accommodate a wider range of logic gates and event dependencies while putting the analyst in control of convergence. To unify these ideas, we use the concept of probabilistic circuits. Here, probability flow is represented as a structured computation graph over Boolean logic, but crucially, each gate’s input distributions come from random samplers rather than a symbolic enumeration of all possible states. This allows the circuit to model both failures and successes while keeping the model size manageable. However, MC approaches for PRA model quantification are not new. The key design principle rests on a massively parallel, hardware accelerated sampling scheme. By encoding component states in compact bit vectors, we exploit hardware-level parallelism. Boolean logic gates, including primitives and complex logic such as K/N translate into efficient bitwise operations. These operations are performed as a computation graph defined by the logic structure of the PRA model itself. We implemented the framework in an open-source tool named Canopy, which is built with SYCL to promote code portability across consumer GPUs, multi-core CPUs, or specialized accelerators like FPGAs.

% \subsection*{The Role of Partial Derivatives}
% One notable benefit of modeling probability distributions over the entire Boolean space is the ability to compute partial derivatives using the Shannon decomposition. Conceptually, the Shannon decomposition defines the derivative of Boolean function f(x), with respect to a subset of inputs. It is efficiently evaluated as a combination of bitwise XOR operations. Our approach samples from computation graphs representing arbitrary Boolean functions, including partial derivatives or more complex operations. This has immediate use for computing not only the conventional Birnbaum importance measure (which indicates how sensitive a system’s failure probability is to a component’s reliability) but also expanding to multi-component subsets. Additionally, since derivatives allow for gradient-based optimizations, there is potential for future work bridging advanced PRA and emerging machine-learning techniques. By embedding an auto-differentiation layer in the MC pipeline, one can imagine learning gate probabilities or even partial system structures from operational data, with the same data-parallel routines driving updates to reliability estimates.

% \subsection*{Limitations}
% \subsubsection*{Sampling Rare Events}
% Ensuring accurate estimates for extremely low-probability events demands careful convergence criteria. In typical nuclear safety analyses, component failures can be vanishingly rare, so polling enough samples to capture these tail probabilities can prove challenging. Techniques such as importance sampling or other variance-reduction strategies may be needed to mitigate the slow convergence that arises when event likelihoods are measured in the range of 10\textsuperscript{-7} or lower. 

% \subsubsection*{Sampling Correlated or Dependent Events}
% A related concern is the treatment of common-cause failures (CCFs) and component dependencies. Characterizing CCF related failures or performing dependency analysis are important PRA activities. Future extensions of this framework thus require sampling from joint distributions, where correlated events are generated as a block. Integrating such dependent draws into bitpacked logic operations is feasible in principle but will require additional data-processing layers to ensure that correlated assignments remain both realistic and efficient to evaluate in large batches.

% \subsection*{When to use this Approach}
% - You want to estimate the probabilites/frequencies of very complex systems without computing minimal cut sets or essential prime implicants.
% - You tried computing the exact probabilities/frequencies using BDDs or ZBDDs and failed.
% - You want an approximate method that converges to the exact solution.
% - You want a streamble solution - you can't wait for an inordinate amount of time to get an answer.

% \subsection*{When to *not* to use this Approach}
% - Your tolerance for the error on a approximate solution is lower than your tolerance for waiting for convergence.

% We have conceptualized this work by taking an unabashedly stochastic-computing stance: every 64-bit unsigned word is treated as 64 independent Bernoulli samples evaluated in parallel. We make no attempt to interpret the word as a multi-bit integer, nor to exploit correlations across bits. This is a deliberate trade between information density for embarrassingly parallel throughput. Each physical bit therefore contributes one Shannon bit of information. Keep that constraint in mind when weighing the solver’s performance claims against its unavoidable precision limits.


% \subsubsection*{Scaling Studies}
% Not all systems benefit from full-scale parallel sampling. For smaller PRA models (fewer components or simpler logic), existing approximate or event exact methods may remain faster and more transparent. A threshold likely exists at which the overhead of distributing computation across GPUs or multi-core CPUs outweighs the parallel advantages. Determining this floor for model size is an important empirical question.
 
% Probabilistic risk assessment (PRA) for nuclear systems relies on analyzing complex Boolean functions that capture failure scenarios spanning thousands of components. PRA quantification workflow involves identifying minimal cut sets and then estimating the probabilities of end states, yet exact methods are intractable for large models because they must evaluate an exponentially growing number of failure combinations. Approximations such as bounding techniques, truncations, logic simplification are often used to curb this computational blow-up, but they restrict model fidelity, omit success paths or subtle dependencies.

% We propose a generalized, data-parallel Monte Carlo (MC) framework that directly estimates probabilities over the entire Boolean space. By operating on input distributions $p(\mathbf{x})$ rather than enumerating minimal cut sets, our SYCL-based approach constructs probabilistic circuits to evaluate $P[f(\mathbf{x})]$ for arbitrary logic gates. This design:  
% 1) Trades exponential summations for an exponentially sampled state space, with the crucial freedom to stop sampling as soon as convergence criteria (user-defined tolerance) are met.  
% 2) Combines integer-bitpacked logic operations with data-parallel hardware acceleration (GPU, FPGA, or multi-core CPU), enabling millions of events to be sampled per second—even on modest consumer hardware.  
% 3) Avoids artificially high error from rare-event truncation by supporting importance sampling and variance reduction methods, improving estimates for extremely low-probability failure modes critical to nuclear safety.  
% 4) Fully decouples probability estimation from cut-set generation, making sense especially for massive PRA models where enumerating minimal cut sets is infeasible or unnecessary.  
% 5) Permits partial derivatives over Boolean functions, computed via Monte Carlo sampling of the Shannon decomposition (XOR events). This functionality underpins advanced sensitivity analyses (e.g., Birnbaum importance) for subsets of basic events—thereby bridging concepts from machine learning’s auto-differentiation into PRA.  

% We acknowledge that naive Monte Carlo can still converge slowly for extremely rare failures; hence, we outline variance reduction techniques and future work on sampling correlated events (e.g., common-cause failures) to address real-world dependencies. We also identify a practical lower bound on PRA model size where traditional methods may outperform our approach and discuss overhead trade-offs in deploying our open-source tool, Canopy, on single-GPU desktops versus distributed HPC clusters. Benchmarks against a generic pressurized water reactor PRA model confirm that, given suitably chosen convergence thresholds, data-parallel sampling can deliver orders-of-magnitude throughput gains relative to industry-standard tools (CAFTA, FTREX, SAPHIRE, SCRAM, XFTA) while maintaining accuracy within nuclear safety tolerances. Ultimately, this framework showcases a scalable path toward unifying tractable probabilistic models, hardware acceleration, and advanced reliability logic for next-generation PRA studies.

% \end{abstract}



% \chapter{A Brief and Informal Overview}
% \section*{Context}
% Probabilistic risk assessment (PRA) for nuclear systems requires the evaluation of complex Boolean functions representing failure scenarios across thousands of components. The evaluation is performed in two separate steps: (i) the identification of the smallest sets of conditional events leading to end states of interest (also called minimal cut sets), and (ii) an estimation of the likelihoods of these end states, given the state of knowledge of individual component reliabilities. Although there are additional steps, PRA quantification is an iterative process that revolves primarily around these tasks since they are computationally burdensome: exact solutions demand exhaustive inclusion-exclusion calculations that grow exponentially with the number of components. Therefore, the analysis of large-scale systems using exact methods remains intractable. 

% In response to these challenges, a generation of tools and methods has organically evolved around various approximations: they often restrict model definitions to a limited set of logic gates (commonly AND and OR), describe only the failure space by enumerating cut sets rather than prime implicants (thus omitting success paths), apply probability truncation schemes such as the rare-event approximation, or employ bounding techniques like the min-cut upper bound (MCUB). In practical terms, PRA quantification remains unwieldy, demanding careful model management and tempered expectations regarding both accuracy and runtime.

% \section*{The Data-Parallel Monte Carlo Approach}
% In response, we propose a generalized, data-parallel framework that directly estimates probability distributions using known Monte-Carlo (MC) techniques. By operating on input probability distributions $P(\mathbf{x})$ rather than symbolic evaluation of $f(\mathbf{x})$, our approach constructs probabilistic circuits that estimate $P[f(\mathbf{x})]$ for Boolean functions, enabling simultaneous quantification of all event sequences in a PRA. In effect, we are trading exponential growth in inclusion-exclusion terms during probability estimation for an exponential number of MC samples, all while making fewer assumptions on the structure of the underlying logic model. By shifting from enumerating minimal cut sets to sampling the entire solution space, our goal is to manage complexity more flexibly, harness modern hardware acceleration, and enable additional features such as partial derivative–based sensitivity analysis.

% Since we can stop sampling at any time based on arbitrary convergence criteria, we can set accuracy targets (and not be limited to using the rare-event truncation). The mean of the expected value still converges to the exact solution, which means, in the general case, the computed probabilities, although still approximations, have less error than when using the rare-event or MCUB approaches.

% The benefits of adopting a sampling approach are many-fold. Event sequence frequencies can be estimated without computing minimal cut sets. This is beneficial when models are so large that BDDs/exact methods cannot be used for probability estimation, and computing minimal cut sets is not required or also infeasible. Sampling also makes it possible to generate correlated samples, which creates a path for future work on CCF and dependency analysis. In addition, using a SYCL-based architecture allows us to make gains in throughput using the latest GPU/FPGA and multi-core CPU hardware. PRA models are integer-bitpacked and logic operations expressed as vectorized bitwise hardware operations, so minimal pre-processing or logic model manipulation is needed. For example, logical K/N gates do not need to be expanded to primitive types, and any combination of logical operations are supported. The actual performance of any logic gate is hardware dependent, which leaves room for future/parallel work on hardware optimizations. The data-parallelism enables the evaluation of billions of events within seconds on older/previous gen consumer hardware. With such abundance of compute, we can perform Boolean derivatives by sampling on the Shannon decomposition. This opens up the ability to define partial derivatives, which we use to perform sensitivity analysis such as calculating importance measures for subsets of basic events. Another emergent effect is that since we can define gradients, the opportunity for parallel/future work on learning model parameters and structures opens up. In sum, by adopting a MC sampling approach, (1) we are able to relax coherence constraints, (2) simultaneously model success and failure, (3) decouple probability estimation from cut set calculation, (4) compute derivatives/gradients by sampling on the Shannon decomposition (XOR), which allows us to (5) perform sensitivity analysis by defining importance measures such as Birnbaum as vectorial derivatives on $f$, (6) all while developing a data-parallel framework and (7) paving the way for finding common ground between the machine-learning (tractable probabilistic model) and PRA communities.

% We demonstrate the stability of our approximation method in our SYCL-based, MIT licensed implementation, Canopy. We benchmark against the generic pressurized water reactor PRA model, comparing its performance and accuracy against industry-standard tools (CAFTA, FTREX, SAPHIRE, SCRAM, and XFTA). 

% Probabilistic risk assessment (PRA) for nuclear systems requires the evaluation of complex Boolean functions representing failure scenarios across thousands of components. The evaluation is performed in two separate steps: (i) the identification of the smallest sets of conditional events leading to end states of interest (also called minimal cut sets), and (ii) an estimation of the likelihoods of these end states, given the state of knowledge of individual component reliabilities. Although there are additional steps, PRA quantification is an iterative process that revolves primarily around these tasks because (a) together, they provide insights that feed into risk reduction activities, and (b), they are the most computationally burdensome because computing exact solutions requires evaluating an exponentially growing number of failure combinations. This means that analysis of large-scale systems using exact methods remains intractable. Existing approaches rely on approximations that restrict model definitions to a subset of logic gates (typically AND, OR logic), operate exclusively in the failure space by describing cut sets rather than prime implicants (often excluding success paths entirely), employ probability truncation schemes like the rare-event approximation, or use bounding techniques such as the min-cut upper-bound (MCUB). In practical terms, PRA analysis remains an unwieldy enterprise, requiring careful model management and sobering expectations of accuracy and runtime.

% In response, we propose a generalized, data-parallel framework that directly estimates probability distributions using known Monte-Carlo (MC) techniques. By operating on input probability distributions $P(\mathbf{x})$ rather than symbolic evaluation of $f(\mathbf{x})$, our approach constructs probabilistic circuits that estimate $P[f(\mathbf{x})]$ for Boolean functions, enabling simultaneous quantification of all event sequences in a PRA. In effect, we are trading exponential growth in inclusion-exclusion terms during probability estimation for an exponential number of MC samples, all while making fewer assumptions on the structure of the underlying logic model. Since we can stop sampling at any time based on arbitrary convergence criteria, we can set accuracy targets (and not be limited to using the rare-event truncation). The mean of the expected value still converges to the exact solution, which means, in the general case, the computed probabilities, although still approximations, have less error than when using the rare-event or MCUB approaches.

% The benefits of adopting a sampling approach are many-fold. Event sequence frequencies can be estimated without computing minimal cut sets. This is beneficial when models are so large that BDDs/exact methods cannot be used for probability estimation, and computing minimal cut sets is not required or also infeasible. Sampling also makes it possible to generate correlated samples, which creates a path for future work on CCF and dependency analysis. In addition, using a SYCL-based architecture allows us to make gains in throughput using the latest GPU/FPGA and multi-core CPU hardware. PRA models are integer-bitpacked and logic operations expressed as vectorized bitwise hardware operations, so minimal pre-processing or logic model manipulation is needed. For example, logical K/N gates do not need to be expanded to primitive types, and any combination of logical operations are supported. The actual performance of any logic gate is hardware dependent, which leaves room for future/parallel work on hardware optimizations. The data-parallelism enables the evaluation of billions of events within seconds on older/previous gen consumer hardware. With such abundance of compute, we can perform Boolean derivatives by sampling on the Shannon decomposition. This opens up the ability to define partial derivatives, which we use to perform sensitivity analysis such as calculating importance measures for subsets of basic events. Another emergent effect is that since we can define gradients, the opportunity for parallel/future work on learning model parameters and structures opens up. In sum, by adopting a MC sampling approach, (1) we are able to relax coherence constraints, (2) simultaneously model success and failure, (3) decouple probability estimation from cut set calculation, (4) compute derivatives/gradients by sampling on the Shannon decomposition (XOR), which allows us to (5) perform sensitivity analysis by defining importance measures such as Birnbaum as vectorial derivatives on $f$, (6) all while developing a data-parallel framework and (7) paving the way for finding common ground between the machine-learning (tractable probabilistic model) and PRA communities.

% We demonstrate the stability of our approximation method in our SYCL-based, MIT licensed implementation, Canopy. We benchmark against the generic pressurized water reactor PRA model, comparing its performance and accuracy against industry-standard tools (CAFTA, FTREX, SAPHIRE, SCRAM, and XFTA). 


%% PRA modeling -> PRA quantification -> 
%% we show it is more than this - sensitivities, etc
%% we approach from a more general framework, relaxing many PRA specific concerns, broadening, 
%% implementing, generalizing, optimizing. And only then, refocusing to PRA related outcomes, and show %% that we have gained a little more insight (we come bearing fruit)... And these are the fruits we bear..

%  Current PRA solvers face computational barriers when analyzing large-scale systems, as they must evaluate an exponentially growing number of failure combinations. To overcome these challenges, existing approaches rely on various approximations: they operate exclusively in the failure space by describing cut sets rather than prime implicants, employ probability truncation schemes like the rare-event approximation, or use bounding techniques such as the min-cut-upper bound. We propose a probabilistic framework that side-steps these challenges by directly estimating probability distributions over Boolean spaces rather than evaluating individual failure combinations.

% Our approach constructs probabilistic circuits that estimate $P[f(\mathbf{x})]$ for Boolean functions, enabling simultaneous quantification of all event sequences and fault trees in a PRA. By operating on input probability distributions $P(\mathbf{x})$ rather than symbolic evaluation of $f(\mathbf{x})$, we avoid the combinatorial explosion inherent in exact calculations. The method employs Monte Carlo sampling in a data-parallel architecture, trading exponential growth in inclusion-exclusion terms for controlled convergence through sampling.

% We demonstrate the stability of our approximation method and extend it to handle composite Boolean operations, including partial derivatives and convolutions through modular circuit design. This enables efficient calculation of key PRA metrics: importance measures can be computed with respect to any subset of success or failure events, not just individual basic events, while essential prime implicants are identified through minimal non-zero gradients in $f$. Our implementation, named Canopy, shows significant performance improvements when tested against the generic pressurized water reactor PRA model. Benchmarks against industry-standard tools (CAFTA, FTREX, SAPHIRE, SCRAM, and XFTA) demonstrate orders-of-magnitude speedup in quantifying sequences with millions of basic events, while maintaining accuracy within established tolerance limits for nuclear safety applications.

% \footnote{Minimal cut sets that include success paths are called essential prime implicants}

% We propose a probabilistic scheme for evaluating Boolean functions with many variables. An immediate application of such an approach is that, since it inherently builds a probability estimator $P[f(\mathbf{x})]$ for Boolean circuits, one is able to quantify all event sequences and logic trees in a probabilistic risk assessment (PRA) simultaneously. Sequences and top events with millions of basic events can be quantified within seconds.

% By developing an estimator-based framework that operates over input probability distributions, we circumvent the need to evaluate $f(\mathbf{x})$ symbolically. Instead, we build circuits to evaluate $P[f(\mathbf{x})]$ for known input probability distributions $P(\mathbf{x})$. By using Monte Carlo sampling, we side-step the combinatorial explosion inherent in the exact probability calculation of $P[f(\mathbf{x})]$. This trades the exponential increase in inclusion-exclusion terms for an exponential increase in the number of Monte Carlo samples, allowing iterative convergence toward the exact solution using a process that is inherently data-parallel.

% After demonstrating that our method for approximating $f(\mathbf{x})$ is stable, we extend our approach to arbitrary combinatorial circuits. Composite Boolean operations, such as partial derivatives and convolutions can be evaluated by cascading modular circuitry. From here on, PRA tasks of high practical value, such as calculating importance measures and performing sensitivity analysis become quite straightforward. Importance measures are expressed as gradient operations with respect to subsets of basic events. The calculation of essential prime implicants, a central problem in PRA quantification, is transformed to determining the smallest subset of $\mathbf{x}$ that results in a non-zero gradient in $f$.

% We benchmark our solver, named Canopy, on the generic pressurized water reactor PRA model, comparing against industry-leading event-tree and fault-tree quantification tools CAFTA, FTREX, SAPHIRE, SCRAM, and XFTA. 


% // note, we are not solving for all X, we don't need to, it's not our problem
% // we care about F when P(x) is bounded, to a certain extent.
% // we want to reframe the problem of evaluating $F(x)\forall x$ as a problem about evaluating $ \hat{F} = P(F); \forall P(x)$
% // the search space is fundamentally large, there is no getting around that
% // but for some problems [list a few types], where $x \in [0, 1]$ some $x$, so we reduce the 
% We propose a data-parallel Monte Carlo scheme for evaluating Boolean functions with many variables, addressing a central practical challenge in risk and reliability analysis. This approach has many benefits, 

% We extend this notion to the general family of Boolean functions, and demonstrate how complex operations, such as partial derivatives and convolutions can be computed efficiently. 

% Instead of symbolically computing a Boolean function $F$, we evaluate $F$ overdo this by developing an estimator-based framework for performing operations on which can be extended for their $n\textsuperscript{th}$ partial derivatives of Boolean expressions, our method effectively circumvents the combinatorial explosion inherent in exact computation techniques. The data-parallel nature of the proposed approach demonstrates exceptional scalability on current-gen consumer hardware.

% An immediate benefit of this approach is that it addresses long-standing needs within Probabilistic Risk Assessment (PRA) model quantification, where existing methods struggle with large-scale models due to computational complexity or scalability limitations. Often, larger models are unquantifiable. Or, they are quantiti 

% By transforming PRA tasks—such as quantifying event sequence probabilities and performing sensitivity analyses—into operations over stochastically computed derivatives of Boolean functions, we enable lower-latency, high-throughput, and accurate model quantification as compared to existing approaches. 
 
% We show how the proposed method is generalizable to a wide array of combinatorial problems across various scientific computing disciplines, including digital circuit design, cryptography, and more. 

