%% ------------------------------ Dissertation Abstract ---------------------------------- %%
%% Dissertation Title:  A Data-Parallel, Hardware-Accelerated Monte Carlo Framework for Quantifying Risk using Probabilistic Circuits
\begin{abstract}
Quantitative risk assessment traditionally relies on exhaustive enumeration of minimal cut sets, a process that grows combinatorially with model size and therefore struggles with the multi-hundred–component systems encountered in contemporary nuclear safety studies. This dissertation introduces a Monte-Carlo paradigm that sidesteps explicit state-space exploration by evaluating the underlying probabilistic directed acyclic graph directly on modern data-parallel hardware, treating the full set of inter-linked event trees, fault trees, and their dependencies as a single unified computation graph. The proposed solver, mcSCRAM, is accompanied by a suite of theoretical and algorithmic advances.  First, we formalize hardware-native gates for threshold and cardinality voting, prove their estimator equivalence to classical AND/OR expansions, and demonstrate exponential savings in graph size and kernel launch overhead for gates with high fan-in. Second, we develop a knowledge compilation pipeline, specifically tailored to Monte Carlo workloads. Third, we derive first-order importance measures and extensions for common-cause failure analysis within the Monte Carlo paradigm. Extensions include a composite convergence criterion that fuses frequentist, Bayesian, and information-theoretic diagnostics, guaranteeing statistical accuracy under user-specified budgets. Formal proofs establish unbiasedness, variance preservation, and convergence rates for all estimators. Comprehensive benchmarks on the 43-model Aralia fault tree dataset show that the compiled graphs, no longer subject to exact inference constraints, can be compressed by a median factor of 1.3×. During simulation, the solver achieves fully saturated device throughput, converging to sub-percent relative error on graphs with a few thousand events in under five seconds—even on entry-level consumer GPUs. The rare-event regime remains challenging but is mitigated by the proposed importance-sampling extension.  Sensitivity studies confirm that randomness quality and convergence diagnostics jointly prevent premature termination. The work demonstrates that data-parallel Monte-Carlo techniques can match the accuracy of exact solvers while scaling to models previously considered intractable. Limitations include residual inefficiency for events with probabilities below $10^{-8}$, reliance on specialized hardware for peak performance, and the need for further validation on full-scale industry PRA benchmarks.  Nevertheless, the methodological foundation laid here opens avenues for stochastic, gradient-based optimization, adaptive variance reduction, and real-time risk monitoring in future research.
\end{abstract}


