\chapter{Current State of PRA Software}
\acrfull{pra} has developed steadily since the 1970s, in parallel with increasing model complexity and advances in computing. Early efforts, including those supporting the Reactor Safety Study (WASH-1400), made clear how hardware constraints dictated the scope and fidelity of probabilistic analyses. Despite steady gains in compute capability, from mainframes to \acrfull{pc}s to modern clusters and cloud services, \acrshort{pra} software has largely adopted incremental improvements, leaving persistent gaps in scalability, usability, and methodological transparency.

Several codes emerged in rapid succession in the field’s early years. PREP and KITT automated fault-tree evaluation via Monte Carlo and deterministic methods; \acrfull{mocus} and SIGPI advanced \acrfull{mcs} generation and system probability calculations. MODULE, RISKMAN, and \acrfull{irras} added capabilities for importance measures, uncertainty handling, and more extensive \acrshort{pc}-based analysis. By the late 1980s, \acrfull{cafta}, \acrfull{saphire}, and RiskSpectrum consolidated many of these functions and became widely adopted.

Contemporary models, covering fire \acrshort{pra}, seismic events, multi-unit sites, and other external hazards, regularly exceed the capacity of legacy single-threaded workflows. Although \acrfull{hpc} techniques, distributed computing, and open-source ecosystems offer paths to greater scalability and interoperability, adoption is uneven. Barriers include commercial and institutional lock-in, limited automation for large-scale model manipulation, and continued difficulty conveying risk insights to non-specialists.

\input{task_I/tables/pra_tools_summary}

\section{An Evolving Computing Landscape}

In the 1970s and 1980s, \acrshort{pra} analyses typically ran as batch jobs on mainframes or minicomputers, with fault-tree quantification consuming hours or days. Early codes (e.g., PREP, KITT, and \acrshort{mocus}) established viable proof-of-concept approaches—Monte Carlo methods, in particular—despite severe memory and processing limits. As desktop computing matured, \acrshort{pra} tools shifted toward locally installed \acrshort{pc} software, reducing dependence on mainframes but inheriting new constraints such as storage limits, slower clock speeds, and constrained user interfaces.

By the end of the 1980s, \acrshort{pra} had expanded to include more intricate regulatory and external-event analyses. Integrated environments began to combine event-tree construction, fault-tree quantification, and uncertainty analysis within a single workflow. However, the prevailing software architectures remained oriented toward single-core execution and local memory.

Today, \acrshort{pra} models increasingly encode multi-hazard interactions (e.g., seismic with fire), site-wide resource couplings across multiple units, and broader parametric and epistemic uncertainties. Without systematic concurrency and distributed execution, quantification becomes prohibitively slow. The near-term requirement is clear: \acrshort{pra} software must fully leverage multicore and distributed platforms.

\section{Persistent Limitations}

\subsection{Scalability}

Although most \acrshort{pra} codes now run on modern operating systems, few scale cleanly to clusters or cloud environments. Even well-established tools (e.g., Phoenix Architect, \acrshort{saphire}, XFTA) largely employ serial quantification with limited concurrency, though this is rapidly changing. Fire or seismic \acrshort{pra} models can include tens of thousands of basic events; tasks such as cut set generation and manipulation push memory demands into the gigabyte range and extend runtimes substantially. In the absence of robust parallelization and memory-aware data structures, analyses can stall or fail during quantification.

\subsection{Model Development}

Building and maintaining \acrshort{pra} models remains labor intensive. Flag-file edits, manual event-tree expansions, and ad hoc data updates can introduce inconsistent states, an acute concern for multi-hazard and dependency analyses. Extending coverage to additional external hazards (e.g., tornado missiles, external flooding, wildfire) often entails substantial re-modeling and is error-prone. While most tools provide \acrshort{gui}s for diagrammatic and tabular editing, the community continues to seek higher levels of automation that support version control, auditing, and systematic scenario extensions.

\subsection{Dependency Analysis}

Capturing dependencies at scale, particularly those involving human actions, remains challenging. \acrfull{hra} dependencies arise from operator interactions with evolving scenarios and are difficult to encode within classical fault trees~\cite{diaconeasa_ads-idac_2017, diaconeasa_branching_2018}. Few tools offer built-in methods for advanced \acrshort{hra} models or cross-system dependencies beyond standard common-cause failures. As a result, risk insights involving operator actions under multi-hazard conditions may rely on oversimplified assumptions.

\subsection{Multi-Hazard, Multi-Unit Modeling}

Modern applications increasingly require multi-unit site analyses. Standard \acrshort{pra} assumptions of unit independence often do not hold; resources such as power, control staff, and emergency cooling may be shared. Incorporating these couplings, along with combined external hazards, can yield combinatorial growth in scenario space. Naïve enumeration of unified hazard representations (e.g., site-level seismic and flooding profiles) can exhaust memory and degrade performance.

\subsection{Communication of Risk Insights}
As models and results grow more sophisticated, communicating those results and their uncertainties to non-\acrshort{pra} stakeholders remains difficult. User-facing interfaces have improved, but effective decision support, particularly for time-sensitive contexts requires intuitive dashboards, real-time updates, and responsive visualization. Closed-source suites often restrict extensibility, complicating the development of tailored analytics.

\subsection{Transparency, Licensing and Community Support}

The most widely used platforms (e.g., \acrshort{saphire}, XFTA, \acrshort{cafta}, RiskSpectrum) are closed-source or only partially accessible. This limits collaborative development and impedes optimization for multicore, cluster, or cloud settings, where concurrency-friendly data structures, partitioned workflows, and specialized libraries may require deep source-level changes. It also slows the adoption of modern practices (containerization, workflow automation, and \acrshort{hpc} libraries), forcing reliance on special arrangements or workarounds.

Open-source efforts such as SCRAM~\cite{scram} and OpenFTA~\cite{opensource_fta_tools} enable community-driven improvement. SCRAM, in particular, provides a flexible toolkit for fault-tree quantification, cut set manipulation, and simplified event-tree analysis.



\section{Looking Forward}
\acrshort{pra} practice is transitioning from single-threaded desktops to scalable, automated, and transparent infrastructures. In the near term, progress will rely on \acrshort{hpc}-oriented frameworks that support parallel quantification, distributed uncertainty propagation, and memory-efficient representations of large correlated event spaces. Over longer horizons, emerging paradigms such as quantum computing and \acrfull{fhe} may enable secure and larger-scale analyses, though at present they are primarily research topics. Sustained advances will depend on integrating parallel computing, reducing manual model-development burden, representing multi-hazard and multi-unit interactions consistently, and delivering risk information that is interpretable and actionable for non-specialists.