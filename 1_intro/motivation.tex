\section{Motivation}
Probabilistic risk assessment (PRA) provides the quantitative substrate on which safety-critical decisions for nuclear installations are made.  From the landmark WASH-1400 study through subsequent regulatory frameworks, its influence has grown in lock-step with the complexity of the engineered systems it seeks to evaluate.  Modern reactor models couple hundreds of event trees with thousands of fault trees, each containing many tens of thousands of basic events.  The Boolean logic underpinning these structures is exacting—but it is also unforgiving: the number of minimal cut sets increases combinatorially with gate fan-in, and the inclusion–exclusion expansions required for exact probability calculations quickly exceed any realistic computational budget.

Historically, analysts have mitigated this hurdle through a hierarchy of approximations: truncating low-order terms, bounding probabilities with min-cut upper limits, or forcing the logic into syntactic fragments that are amenable to binary-decision diagrams.  These techniques, implemented in widely deployed tools such as FTREX, SAPHIRE, SCRAM, XFTA, and RiskSpectrum, remain workhorses of industry practice.  Yet each approximation trades rigor for tractability, often at the cost of conservatism or opacity -- an increasingly unattractive compromise as regulatory emphasis shifts toward risk-informed, performance-based licensing.

A different path has opened in the last decade.  The proliferation of many-core GPUs, coupled with mature programming models, enables billions of independent arithmetic operations per second on commodity hardware.  Monte Carlo (MC) simulation is naturally aligned with such architectures: because individual samples are statistically independent, they map cleanly onto the embarrassingly parallel compute model of modern accelerators.  Sampling the global state of a PRA model therefore offers a conceptually simple alternative to symbolic enumeration, provided that three longstanding obstacles can be overcome: (i) efficient generation of high-quality random events at scale, (ii) evaluation of large Boolean circuits at hardware line-rates, and (iii) principled convergence diagnostics that certify the statistical quality of the resulting estimates.

This dissertation tackles those obstacles head-on.  It introduces a data-parallel Monte Carlo framework that compresses system states into machine-word bit-vectors, evaluates entire layers of the PRA logic in a single pass, and embeds rigorous stopping rules that blend frequentist and Bayesian error metrics.  By revisiting PRA quantification from a hardware-conscious perspective we aim not merely to *approximate* classical methods but to redefine the achievable trade-space between fidelity and turnaround time.  The developments that follow build the theoretical, algorithmic and empirical foundations for that goal.

\section{Scope \& Objectives}
This dissertation seeks to rethink quantitative risk assessment from a hardware-conscious, data-parallel perspective.  Its specific objectives are:

\begin{enumerate}[label=\textbf{O\arabic*}.,leftmargin=2em]
  \item \textbf{Methodological foundation.}  Devise a Monte-Carlo workflow that evaluates complete PRA models—including all inter-linked event trees and fault trees—without minimal-cut enumeration or logic simplifications.
  \item \textbf{Algorithmic and data-structure design.}  Create compilation, storage, and kernel-evaluation strategies that exploit many-core GPUs, multi-core CPUs, and reconfigurable hardware while remaining portable across architectures.
  \item \textbf{Robust statistical machinery.}  Extend the baseline sampler to handle domain-specific challenges—rare events, common-cause failures, and importance derivatives—and embed composite convergence diagnostics with provable error guarantees.
  \item \textbf{Empirical validation.}  Benchmark the resulting framework on public PRA datasets, reporting reproducible metrics for accuracy, runtime, and memory footprint, and compare its fidelity and performance to established exact and approximate tools.
\end{enumerate}

Although the primary test bed is nuclear safety analysis, the techniques generalize to any industry that models complex Boolean risk scenarios (e.g.\, aerospace, chemical processing, autonomous vehicles).  Dynamic simulations and strongly correlated uncertainty models lie outside the immediate scope; however, the architecture is deliberately extensible so that such capabilities can be integrated in future work.